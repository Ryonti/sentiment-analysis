{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yonti's\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('G0OgL3e!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hari ini aku masih libur, masih ada waktu semi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yg ptm hari ini semangaattt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dom cilacap udh ptm 100% apa blm?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yg dom jabodetabek kalian ptm nya udah 100% kah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yg ptm yg genap seru bgt, yg ganjil kek nya ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  hari ini aku masih libur, masih ada waktu semi...\n",
       "1                        yg ptm hari ini semangaattt\n",
       "2                  dom cilacap udh ptm 100% apa blm?\n",
       "3   yg dom jabodetabek kalian ptm nya udah 100% kah?\n",
       "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/small.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                 text  \\\n",
      "0  hari ini aku masih libur, masih ada waktu semi...   \n",
      "1                        yg ptm hari ini semangaattt   \n",
      "2                  dom cilacap udh ptm 100% apa blm?   \n",
      "3   yg dom jabodetabek kalian ptm nya udah 100% kah?   \n",
      "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba...   \n",
      "5             wtp hr ini gue libur tp BESOK PTM 100%   \n",
      "\n",
      "                                          lower_text  \\\n",
      "0  hari ini aku masih libur, masih ada waktu semi...   \n",
      "1                        yg ptm hari ini semangaattt   \n",
      "2                  dom cilacap udh ptm 100% apa blm?   \n",
      "3   yg dom jabodetabek kalian ptm nya udah 100% kah?   \n",
      "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba...   \n",
      "5             wtp hr ini gue libur tp besok ptm 100%   \n",
      "\n",
      "                                       remove_number  \\\n",
      "0  hari ini aku masih libur, masih ada waktu semi...   \n",
      "1                        yg ptm hari ini semangaattt   \n",
      "2                  dom cilacap udh ptm 100% apa blm?   \n",
      "3   yg dom jabodetabek kalian ptm nya udah 100% kah?   \n",
      "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba...   \n",
      "5             wtp hr ini gue libur tp besok ptm 100%   \n",
      "\n",
      "                                         punctuation  \\\n",
      "0  hari ini aku masih libur, masih ada waktu semi...   \n",
      "1                        yg ptm hari ini semangaattt   \n",
      "2                  dom cilacap udh ptm 100% apa blm?   \n",
      "3   yg dom jabodetabek kalian ptm nya udah 100% kah?   \n",
      "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba...   \n",
      "5             wtp hr ini gue libur tp besok ptm 100%   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [hari, ini, aku, masih, libur, ,, masih, ada, ...   \n",
      "1                  [yg, ptm, hari, ini, semangaattt]   \n",
      "2      [dom, cilacap, udh, ptm, 100, %, apa, blm, ?]   \n",
      "3  [yg, dom, jabodetabek, kalian, ptm, nya, udah,...   \n",
      "4  [yg, ptm, yg, genap, seru, bgt, ,, yg, ganjil,...   \n",
      "5  [wtp, hr, ini, gue, libur, tp, besok, ptm, 100...   \n",
      "\n",
      "                                          freq_token  \\\n",
      "0  {'hari': 2, 'ini': 3, 'aku': 2, 'masih': 2, 'l...   \n",
      "1  {'yg': 1, 'ptm': 1, 'hari': 1, 'ini': 1, 'sema...   \n",
      "2  {'dom': 1, 'cilacap': 1, 'udh': 1, 'ptm': 1, '...   \n",
      "3  {'yg': 1, 'dom': 1, 'jabodetabek': 1, 'kalian'...   \n",
      "4  {'yg': 3, 'ptm': 1, 'genap': 1, 'seru': 1, 'bg...   \n",
      "5  {'wtp': 1, 'hr': 1, 'ini': 1, 'gue': 1, 'libur...   \n",
      "\n",
      "                                             stemmed  \\\n",
      "0  hari ini aku masih libur, masih ada waktu semi...   \n",
      "1                        yg ptm hari ini semangaattt   \n",
      "2                  dom cilacap udh ptm 100% apa blm?   \n",
      "3   yg dom jabodetabek kalian ptm nya udah 100% kah?   \n",
      "4  yg ptm yg genap seru bgt, yg ganjil kek nya ba...   \n",
      "5             wtp hr ini gue libur tp besok ptm 100%   \n",
      "\n",
      "                                            textblob  sentiment  \n",
      "0  (h, a, r, i,  , i, n, i,  , a, k, u,  , m, a, ...        1.0  \n",
      "1  (y, g,  , p, t, m,  , h, a, r, i,  , i, n, i, ...        1.0  \n",
      "2  (d, o, m,  , c, i, l, a, c, a, p,  , u, d, h, ...        1.0  \n",
      "3  (y, g,  , d, o, m,  , j, a, b, o, d, e, t, a, ...        1.0  \n",
      "4  (y, g,  , p, t, m,  , y, g,  , g, e, n, a, p, ...        1.0  \n",
      "5  (w, t, p,  , h, r,  , i, n, i,  , g, u, e,  , ...        1.0  >\n"
     ]
    }
   ],
   "source": [
    "df['lower_text'] = df['text'].str.lower()  # Convert to lowercase\n",
    "df['remove_number'] = df['lower_text'].str.replace(r'\\d+', '') # Remove number\n",
    "df['punctuation'] = df['remove_number'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n",
    "df['tokenized_text'] = df['punctuation'].apply(nltk.word_tokenize) # Tokenize the text\n",
    "df['freq_token'] = df['tokenized_text'].apply(nltk.FreqDist).apply(lambda x: dict(x)) # Frequency word token\n",
    "df['stemmed'] = df['punctuation'].apply(lambda x: ' '.join(nltk.corpus.stopwords.words('indonesia') if x.lower() in nltk.corpus.stopwords.words('english') else [x]))\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()   # Convert text to lower\n",
    "\n",
    "token_text = re.sub(r\"\\d+\", \"\", lower_text)    # Remove number\n",
    "token_text = re.sub('\\s+',' ', token_text) # Remove multiple whitespace into single whitespace\n",
    "token_text = ''.join(c for c in token_text if c not in string.punctuation)      # Remove punctuation\n",
    "word_tokens = nltk.tokenize.word_tokenize(token_text)    # Tokenize the text\n",
    "freq_tokens = nltk.FreqDist(word_tokens)    # Frequency word token\n",
    "\n",
    "stop_words = [word for word in word_tokens if word not in stopwords.words('english')]   # Implement stopwords\n",
    "    \n",
    "stemmer = nltk.stem.SnowballStemmer('english')  # Initialized stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in stop_words]   # Stem the words\n",
    "\n",
    "normalized_text = ''.join(stemmed_words)  # Join the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : g0ogl3e!\n",
      "Remove punctuation, number, multiple whitespace : gogle\n",
      "Tokenizing Result :  ['gogle']\n",
      "Frequency Token :  [('gogle', 1)]\n",
      "Stopword :  ['gogle']\n",
      "Stemmer :  ['gogl']\n",
      "Normalized :  gogl\n"
     ]
    }
   ],
   "source": [
    "print(f'Case Folding Result : {lower_text}')\n",
    "print(f'Remove punctuation, number, multiple whitespace : {token_text}')\n",
    "print('Tokenizing Result : ',word_tokens)\n",
    "print('Frequency Token : ',freq_tokens.most_common())\n",
    "print('Stopword : ',stop_words)\n",
    "print('Stemmer : ',stemmed_words)\n",
    "print('Normalized : ',normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "5    1.0\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['textblob'] = df['punctuation'].apply(lambda x: TextBlob(x))\n",
    "\n",
    "# Create a new column in the DataFrame with sentiment analysis scores\n",
    "df['sentiment'] = df['textblob'].apply(lambda x: x.sentiment.polarity)\n",
    "\n",
    "# Normalize the sentiment scores to range from -1 (negative) to 1 (positive)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: np.interp(x, [df['sentiment'].min(), df['sentiment'].max()], [-1, 1]))\n",
    "\n",
    "print(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral sentiment\n"
     ]
    }
   ],
   "source": [
    "analysis = TextBlob(normalized_text)\n",
    "\n",
    "# Get sentiment polarity (-1 to 1: negative to positive)\n",
    "sentiment = analysis.sentiment.polarity\n",
    "if sentiment > 0: \n",
    "    print(\"Positive sentiment\")\n",
    "elif sentiment < 0: \n",
    "    print(\"Negative sentiment\")\n",
    "else: \n",
    "    print(\"Neutral sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yonti's\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello google\n"
     ]
    }
   ],
   "source": [
    "text = input(\"TEXT = \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hari ini aku masih libur, masih ada waktu semi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yg ptm hari ini semangaattt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yg lagi ptm sini ciss dulu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yang ptm semangat ya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yang ptm selamat menikmati hari bahagia kalian ya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  hari ini aku masih libur, masih ada waktu semi...          1\n",
       "1                        yg ptm hari ini semangaattt          1\n",
       "2                         yg lagi ptm sini ciss dulu          1\n",
       "3                               yang ptm semangat ya          1\n",
       "4  yang ptm selamat menikmati hari bahagia kalian ya          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/twitter.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    lower_text = text.lower()   # Convert text to lower\n",
    "\n",
    "    token_text = re.sub(r\"\\d+\", \"\", lower_text)    # Remove number\n",
    "    token_text = re.sub('\\s+',' ', token_text) # Remove multiple whitespace into single whitespace\n",
    "    token_text = ''.join(c for c in token_text if c not in string.punctuation)      # Remove punctuation\n",
    "    word_tokens = nltk.tokenize.word_tokenize(token_text)    # Tokenize the text\n",
    "    freq_tokens = nltk.FreqDist(word_tokens)    # Frequency word token\n",
    "\n",
    "    stop_words = [word for word in word_tokens if word not in stopwords.words('english')]   # Implement stopwords\n",
    "        \n",
    "    stemmer = nltk.stem.SnowballStemmer('english')  # Initialized stemmer\n",
    "    stemmed_words = [stemmer.stem(word) for word in stop_words]   # Stem the words\n",
    "\n",
    "    normalized_text = ''.join(stemmed_words)  # Join the words\n",
    "\n",
    "\"\"\"\n",
    "print('Case Folding Result : ') \n",
    "print(lower_text)\n",
    "\n",
    "print('\\nRemove punctuation, number, multiple whitespace : ')\n",
    "print(token_text)\n",
    "\n",
    "print('\\nTokenizing Result : ') \n",
    "print(word_tokens)\n",
    "\n",
    "print('\\nFrequency Token : ')\n",
    "print(freq_tokens.most_common())\n",
    "\n",
    "print('\\nStopword : ')\n",
    "print(stop_words)\n",
    "\n",
    "print('\\nStemmer : ')\n",
    "print(stemmed_words)\n",
    "\n",
    "print('\\nNormalized : ')\n",
    "print(normalized_text)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, lang):\n",
    "    \"\"\"\n",
    "    Preprocesses the text in the 'review' column of the given DataFrame.\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x: preprocess(x, lang))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(df, 'english')\n",
    "\n",
    "# Create Tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Convert the 'review' column into its TF-IDF vectorized form\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train and Evaluate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral sentiment\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "# text = \"I love this product. It's amazing!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "analysis = TextBlob(normalized_text)\n",
    "\n",
    "# Get sentiment polarity (-1 to 1: negative to positive)\n",
    "sentiment = analysis.sentiment.polarity\n",
    "if sentiment > 0: \n",
    "    print(\"Positive sentiment\")\n",
    "elif sentiment < 0: \n",
    "    print(\"Negative sentiment\")\n",
    "else: \n",
    "    print(\"Neutral sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert training and test datasets into vectorized form\n",
    "X_train_vectorized, vectorizer = tfidf_vectorizer(df)\n",
    "X_test_vectorized, = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naive Bayes model\n",
    "model = MultinomialNB(alpha=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict sentiment for test dataset\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
    "print(f'Accuracy of the model: {accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
